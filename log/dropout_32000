# conda environments:
#
humor                 *  /home/jzhong9/.conda/envs/humor
base                     /software/anaconda/2018.12b

Mon Jul  8 10:35:00 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.72       Driver Version: 410.72       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:08:00.0 Off |                    0 |
| N/A   50C    P0   113W / 149W |    157MiB / 11441MiB |     56%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K80           Off  | 00000000:09:00.0 Off |                    0 |
| N/A   53C    P0    83W / 149W |   4494MiB / 11441MiB |     42%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla K80           Off  | 00000000:88:00.0 Off |                    0 |
| N/A   48C    P0   126W / 149W |    200MiB / 11441MiB |     92%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla K80           Off  | 00000000:89:00.0 Off |                    0 |
| N/A   49C    P0    85W / 149W |      0MiB / 11441MiB |     41%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     18298      C   python                                       146MiB |
|    1     15684      C   python                                      4483MiB |
|    2      8928      C   pmemd.cuda                                   189MiB |
+-----------------------------------------------------------------------------+
cannot import Tensorboard, use pickle for logging
[*] Using CUDA 1 devices
Seeds initialized to 2
<_io.TextIOWrapper name='cfg/PASE_dense.cfg' mode='r' encoding='UTF-8'>
{'kwidths': [251, 20, 11, 11, 11, 11, 11, 11], 'strides': [1, 10, 2, 1, 2, 1, 2, 2], 'fmaps': [64, 64, 128, 128, 256, 256, 512, 512], 'emb_dim': 100, 'denseskips': True, 'norm_out': True}
Compose(
    ToTensor()
    MIChunkWav(32000)
    LPS(n_fft=2048, hop=160, win=400, device=cpu)
    MFCC(order=20, sr=16000)
    Prosody(hop=160, win=400, f0_min=60, f0_max=300, sr=16000)
    ZNorm(/scratch/jzhong9/data/LibriSpeech_50h/librispeech_50h_stats.pkl)
)
Found 2484 speakers info
Found 55134 files in train split
Found 2484 speakers in train split
Found 2484 speakers info
Found 6125 files in valid split
Found 0 speakers in valid split
False
training pase...
==================================================
name spc
==================================================
num_inputs:  100
ctxt_frames:  5
num_inputs:  600
==================================================
name mi
==================================================
==================================================
name cmi
==================================================
Using poly LR Scheduler for frontend!
Using poly LR Scheduler for spc!
Using poly LR Scheduler for mi!
Using poly LR Scheduler for cmi!
Using poly LR Scheduler for chunk!
Using poly LR Scheduler for lps!
Using poly LR Scheduler for mfcc!
Using poly LR Scheduler for prosody!
Use tenoserboard: False
dropout
droping workers with rate: 0.5
Warning: did not find a list of blocks...
Just printing all params calculation.
BaseModel total params: 5958820
Frontend params:  5958820
==================================================
Beginning training...
Batches per epoch:  2506
Loss schedule policy: dropout
  0%|          | 0/2506 [00:00<?, ?it/s]Epoch 0/150:   0%|          | 0/2506 [00:00<?, ?it/s]Epoch 0/150:   0%|          | 1/2506 [00:15<10:32:52, 15.16s/it]Epoch 0/150:   0%|          | 1/2506 [00:15<10:32:52, 15.16s/it]Epoch 0/150:   0%|          | 2/2506 [00:17<7:52:00, 11.31s/it] Epoch 0/150:   0%|          | 2/2506 [00:17<7:52:00, 11.31s/it]Epoch 0/150:   0%|          | 3/2506 [00:20<6:07:22,  8.81s/it]Epoch 0/150:   0%|          | 3/2506 [00:20<6:07:22,  8.81s/it]Epoch 0/150:   0%|          | 4/2506 [00:22<4:45:51,  6.86s/it]Epoch 0/150:   0%|          | 4/2506 [00:22<4:45:51,  6.86s/it]Epoch 0/150:   0%|          | 5/2506 [00:25<3:52:52,  5.59s/it]Epoch 0/150:   0%|          | 5/2506 [00:25<3:52:52,  5.59s/it]Epoch 0/150:   0%|          | 6/2506 [00:28<3:20:28,  4.81s/it]Epoch 0/150:   0%|          | 6/2506 [00:28<3:20:28,  4.81s/it]Epoch 0/150:   0%|          | 7/2506 [00:31<2:57:42,  4.27s/it]Epoch 0/150:   0%|          | 7/2506 [00:31<2:57:42,  4.27s/it]Epoch 0/150:   0%|          | 8/2506 [00:33<2:32:54,  3.67s/it]Epoch 0/150:   0%|          | 8/2506 [00:33<2:32:54,  3.67s/it]Epoch 0/150:   0%|          | 9/2506 [00:36<2:24:26,  3.47s/it]Epoch 0/150:   0%|          | 9/2506 [00:36<2:24:26,  3.47s/it]Epoch 0/150:   0%|          | 10/2506 [00:39<2:18:15,  3.32s/it]Epoch 0/150:   0%|          | 10/2506 [00:39<2:18:15,  3.32s/it]Epoch 0/150:   0%|          | 11/2506 [00:41<2:05:25,  3.02s/it]Epoch 0/150:   0%|          | 11/2506 [00:41<2:05:25,  3.02s/it]Epoch 0/150:   0%|          | 12/2506 [00:44<1:58:43,  2.86s/it]Epoch 0/150:   0%|          | 12/2506 [00:44<1:58:43,  2.86s/it]Epoch 0/150:   1%|          | 13/2506 [00:46<1:51:47,  2.69s/it]Epoch 0/150:   1%|          | 13/2506 [00:46<1:51:47,  2.69s/it]Epoch 0/150:   1%|          | 14/2506 [00:49<1:55:17,  2.78s/it]Epoch 0/150:   1%|          | 14/2506 [00:49<1:55:17,  2.78s/it]Epoch 0/150:   1%|          | 15/2506 [00:52<1:58:00,  2.84s/it]Epoch 0/150:   1%|          | 15/2506 [00:52<1:58:00,  2.84s/it]Epoch 0/150:   1%|          | 16/2506 [00:55<1:59:49,  2.89s/it]Epoch 0/150:   1%|          | 16/2506 [00:55<1:59:49,  2.89s/it]Epoch 0/150:   1%|          | 17/2506 [00:58<2:01:12,  2.92s/it]Epoch 0/150:   1%|          | 17/2506 [00:58<2:01:12,  2.92s/it]Epoch 0/150:   1%|          | 18/2506 [01:01<2:02:07,  2.95s/it]Epoch 0/150:   1%|          | 18/2506 [01:01<2:02:07,  2.95s/it]Epoch 0/150:   1%|          | 19/2506 [01:02<1:38:29,  2.38s/it]Epoch 0/150:   1%|          | 19/2506 [01:02<1:38:29,  2.38s/it]
Traceback (most recent call last):
  File "train.py", line 224, in <module>
    train(opts)
  File "train.py", line 145, in train
    Trainer.train_(dloader, device=device, valid_dataloader=va_dloader)
  File "/gpfs/fs1/home/jzhong9/pase/pase/models/WorkerScheduler/trainer.py", line 169, in train_
    h, chunk, preds, labels = self.model.forward(batch, device)
  File "/gpfs/fs1/home/jzhong9/pase/pase/models/pase.py", line 243, in forward
    h, chunk = self.frontend(x, device)
  File "/home/jzhong9/.conda/envs/humor/lib/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/gpfs/fs1/home/jzhong9/pase/pase/models/WorkerScheduler/encoder.py", line 22, in forward
    y = self.frontend(x)
  File "/home/jzhong9/.conda/envs/humor/lib/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/gpfs/fs1/home/jzhong9/pase/pase/models/frontend.py", line 119, in forward
    h = block(h)
  File "/home/jzhong9/.conda/envs/humor/lib/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/gpfs/fs1/home/jzhong9/pase/pase/models/modules.py", line 824, in forward
    h = forward_norm(h, self.norm)
  File "/gpfs/fs1/home/jzhong9/pase/pase/models/modules.py", line 31, in forward_norm
    return norm_layer(x)
  File "/home/jzhong9/.conda/envs/humor/lib/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jzhong9/.conda/envs/humor/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/jzhong9/.conda/envs/humor/lib/python3.7/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: CUDA out of memory. Tried to allocate 750.00 MiB (GPU 0; 11.17 GiB total capacity; 9.03 GiB already allocated; 380.31 MiB free; 1.44 GiB cached)
